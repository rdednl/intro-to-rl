\chapter{Conclusions and Future Works}
\label{chapter5}

\vspace{0.5cm}

\noindent After four months of continuous researching and exploring of the reinforcement learning field, it is essential to draw some conclusions and think about future works.

An important consideration to make is that at the beginning, I knew nothing about reinforcement learning and it has been an incredible exploration of the subject.

Putting hands on the universe-starter-agent and managing a high diversity of environments has been very helpful for the understanding of the subject. 

Hyper-parameter selection is, as always, a cumbersome task and finding the correct set up, brute forcing the initialization, is possible only for big companies that have a lot of computing power. Still, there is no clear explanation of why some parameters are set in a certain way and why a Convnet or an LSTM must be built in a given way. 

Future works certainly include a deeper understanding of different algorithms and implementation of improved function approximators. \\
A suggestion comes directly from DeepMind that considers Schulman et al.'s generalized advantage estimation~\cite{schulman2015} to improve the estimation of the advantage function.

Certainly this field is one of the most actives in research groups recently, being that it could be improved in so many ways and that hypothetically could be used in so many applications, from self-driving cars to robotics, from inventory and delivery management to financial pricing and much more.